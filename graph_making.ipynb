{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "data_0 = pd.read_csv('../Data/imputed_data_new_3.csv')",
   "metadata": {
    "collapsed": false
   },
   "id": "f9078488c53a5fbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_0.isna().sum().sum()",
   "id": "a28564a922ae52e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort by datadate\n",
    "\n",
    "data_0['datadate'] = pd.to_datetime(data_0['datadate'])\n",
    "data_0 = data_0.sort_values(by='datadate').reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67d1939f1345a63f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ratings = sorted(list(data_0[\"pastrating\"].unique()))\n",
    "ratings_to_idx = {rating: i for i, rating in enumerate(ratings)}\n",
    "data_0[\"pastrating\"] = data_0[\"pastrating\"].apply(lambda x: ratings_to_idx[x])"
   ],
   "id": "4d2dc0636509c8f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data_0.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39538e883237038a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "synth_tics = set(data_0[\"tic\"].unique()) - set(data_0[data_0[\"year\"].isin(range(2010, 2018))][\"tic\"].unique())\n",
    "len(synth_tics)"
   ],
   "id": "f0db19354e449715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_before_2018 = data_0[data_0['year'] < 2018]\n",
    "data_after_2017 = data_0[data_0['year'] >= 2018]\n",
    "data_after_2017[data_after_2017['tic'].isin(synth_tics)][['tic', 'year', 'quarter', 'pastrating']]"
   ],
   "id": "d2f7c401452a5dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Duplicates?\n",
    "\n",
    "data_0.duplicated().sum()"
   ],
   "id": "ecc82bb6b2a604e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get binary columns\n",
    "\n",
    "binary_cols = data_0.columns[data_0.nunique() == 2]\n",
    "binary_cols"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4214884b4e00761f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get object columns\n",
    "\n",
    "object_cols = data_0.select_dtypes(include='object').columns\n",
    "object_cols"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d470211324ba12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print all the dtypes\n",
    "\n",
    "data_0.dtypes"
   ],
   "id": "80271aa5fa4666e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert int64 columns to float64\n",
    "\n",
    "int_cols = data_0.select_dtypes(include='int64').columns\n",
    "data_0[int_cols] = data_0[int_cols].astype('float64')"
   ],
   "id": "621ee125e448e4ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_0.dtypes",
   "id": "3ab94fd8e009f70e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for extreme values\n",
    "\n",
    "data_0.describe()"
   ],
   "id": "e9776d6f2303d1a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Standardize all columns except binary columns, object columns, datadate, year, quarter, pastrating\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_1 = data_0.copy()\n",
    "\n",
    "data_1 = data_1.drop(columns=binary_cols)\n",
    "data_1 = data_1.drop(columns=object_cols)\n",
    "data_1 = data_1.drop(columns=['datadate', 'year', 'quarter', 'pastrating'])\n",
    "\n",
    "data_1 = pd.DataFrame(scaler.fit_transform(data_1), columns=data_1.columns)\n",
    "data_1 = pd.concat([data_0[binary_cols], data_0[object_cols], data_0[['datadate', 'year', 'quarter', 'pastrating']], data_1], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9cc9757b5ef323d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data_1.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ee572ac530cc32f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for duplicates\n",
    "\n",
    "data_1.duplicated().sum()"
   ],
   "id": "53498664f4fdc37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove duplicates\n",
    "\n",
    "data_1 = data_1.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc597efa398e6af9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# save as csv\n",
    "\n",
    "# data_1.to_csv(\"../Data/standardized_data_new.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a2ef2aeb7ed09f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dtypes = {\n",
    "    \"date\": \"str\",\n",
    "    \"TICKER\": \"str\",\n",
    "    \"PRC\": \"float\"\n",
    "}\n",
    "\n",
    "cols = list(dtypes.keys())\n",
    "\n",
    "stock_data_original = pd.read_csv(\"../Data/stock_data.csv\", dtype=dtypes,  usecols=cols, engine=\"c\", parse_dates=[\"date\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1c05c4edc4f5d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data_original.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1158f09084dec025",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data_original[\"TICKER\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b7cf3d6010fff4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(set(data_1[\"tic\"].unique()) - set(stock_data_original[\"TICKER\"].unique()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345bda3582c0b76a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data_1[\"tic\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc9e7d68eb121ea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data_1 = stock_data_original.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5dcc87d06ef8f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data_1 = stock_data_1.dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9b1ad83a4e627d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove tics that are not in data_1\n",
    "\n",
    "stock_data_1 = stock_data_1[stock_data_1[\"TICKER\"].isin(data_1[\"tic\"].unique())]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2532a7361b76afdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# to datetime\n",
    "\n",
    "stock_data_1[\"date\"] = pd.to_datetime(stock_data_1[\"date\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b651f3326bbcfbaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data_1 = stock_data_1.rename(columns={\"TICKER\": \"tic\", \"date\": \"dater\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "761983b76b2c45ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create year and quarter columns\n",
    "\n",
    "stock_data_1[\"year\"] = stock_data_1[\"dater\"].dt.year\n",
    "stock_data_1[\"quarter\"] = stock_data_1[\"dater\"].dt.quarter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90340140ddbb8033",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort by date\n",
    "\n",
    "stock_data_1 = stock_data_1.sort_values(by=\"dater\").reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5241ba4a3ae8ed51",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data_1.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f23001ca45ead761",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    " # Check if all quarters are are present for each year for each tic\n",
    "\n",
    "for year in range(2010, 2021):\n",
    "    for tic in data_1[\"tic\"].unique():\n",
    "        quarters = data_1[(data_1['year'] == year) & (data_1['tic'] == tic)]['quarter'].unique()\n",
    "        if len(quarters) != 4:\n",
    "            print(f\"Year: {year}, Tic: {tic}, quarters: {data_1[(data_1['year'] == year) & (data_1['tic'] == tic)]['quarter'].unique()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d9ff765543d37e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    " # Check if all quarters are are present for each year for each tic\n",
    "\n",
    "for year in range(2010, 2021):\n",
    "    for tic in stock_data_1[\"tic\"].unique():\n",
    "        quarters = stock_data_1[(stock_data_1['year'] == year) & (stock_data_1['tic'] == tic)]['quarter'].unique()\n",
    "        if len(quarters) != 4:\n",
    "            print(f\"Year: {year}, Tic: {tic}, quarters: {stock_data_1[(stock_data_1['year'] == year) & (stock_data_1['tic'] == tic)]['quarter'].unique()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15144127382b07d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "stock_data = stock_data_1.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13133a212d393045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming stock_data_1 is defined elsewhere and contains the necessary data\n",
    "stock_data = stock_data_1.copy()\n",
    "\n",
    "# Ensure 'dater' is a datetime column\n",
    "stock_data['dater'] = pd.to_datetime(stock_data['dater'])\n",
    "\n",
    "# Sort the DataFrame by date and ticker\n",
    "stock_data.sort_values(['dater', 'tic'], inplace=True)\n",
    "\n",
    "# Assuming 'PRC' is the column for which you want to calculate the standard deviation\n",
    "# Create a 'year_quarter' column to help in grouping\n",
    "stock_data['year_quarter'] = stock_data['year'].astype(str) + 'Q' + stock_data['quarter'].astype(str)\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "sps_dict = {}\n",
    "\n",
    "# Function to calculate rolling standard deviation for 365 days with a 30-day window\n",
    "def calculate_rolling_std(df):\n",
    "    # Ensure there are no duplicate dates\n",
    "    df = df.drop_duplicates(subset=\"dater\", keep=\"first\")\n",
    "    # Fill in missing dates (if any), assuming stock data is daily\n",
    "    df = df.set_index('dater').asfreq('D').reset_index()\n",
    "    # Forward fill the missing values in 'PRC'\n",
    "    df['PRC'] = df['PRC'].ffill()\n",
    "    # Calculate the 30-day rolling standard deviation\n",
    "    df['PRC_std_30'] = df['PRC'].rolling(window=30, min_periods=1).std()\n",
    "    return df['PRC_std_30']\n",
    "\n",
    "# Apply the function to each group and store the result in sps_dict\n",
    "for (tic, year_quarter), group in stock_data.groupby(['tic', 'year_quarter']):\n",
    "    sps_dict.setdefault(year_quarter, {})[tic] = calculate_rolling_std(group).to_numpy()\n",
    "\n",
    "# Now, sps_dict has the structure: { tic: { year_quarter: array_of_365_day_std_dev, ... }, ... }\n",
    "\n",
    "# Example on how to access the data:\n",
    "# print(sps_dict['AAPL']['2021Q1'])\n"
   ],
   "id": "80e65059596a234e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# \n",
    "# stock_data = stock_data_1.copy()\n",
    "# \n",
    "# # Ensure 'dater' is a datetime column\n",
    "# stock_data['dater'] = pd.to_datetime(stock_data['dater'])\n",
    "# \n",
    "# # Sort the DataFrame by date and ticker\n",
    "# stock_data.sort_values(['dater', 'tic'], inplace=True)\n",
    "# \n",
    "# # Assuming 'PRC' is the column for which you want to calculate the standard deviation\n",
    "# # Create a 'year_quarter' column to help in grouping\n",
    "# stock_data['year_quarter'] = stock_data['year'].astype(str) + 'Q' + stock_data['quarter'].astype(str)\n",
    "# \n",
    "# # Initialize a dictionary to store the results\n",
    "# sps_dict = {}\n",
    "# \n",
    "# # Function to calculate rolling standard deviation and keep only the last 30 values\n",
    "# def calculate_rolling_std(df):\n",
    "#     # drop duplicates\n",
    "#     df = df.drop_duplicates(subset=\"dater\", keep=\"first\")\n",
    "#     # sort by dater\n",
    "#     df = df.sort_values(by=\"dater\")\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     df = df.set_index('dater').asfreq(\"D\", method=\"ffill\").reset_index()\n",
    "#     df['PRC_std_30'] = df['PRC'].rolling(window=30, min_periods=1).std()\n",
    "#     return df.iloc[-30:]['PRC_std_30']  # Assuming we want to keep the last 30 days of std dev\n",
    "# \n",
    "# # Apply the function to each group and store the result in sps_dict\n",
    "# for (tic, year_quarter), group in stock_data.groupby(['tic', 'year_quarter']):\n",
    "#     #sps_dict[year_quarter][tic] = calculate_rolling_std(group).to_numpy()\n",
    "#     sps_dict.setdefault(year_quarter, {})[tic] = calculate_rolling_std(group).to_numpy()\n",
    "# \n",
    "# # Now, sps_dict has the structure: { (tic, year_quarter): Series_of_30_day_std_dev, ... }\n",
    "# \n",
    "# # Example on how to access the data:\n",
    "# # print(sps_dict[('AAPL', '2021Q1')])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90ce442efc11d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# for key in sps_dict.keys():\n",
    "#     for tic in sps_dict[key].keys():\n",
    "#         if np.isnan(sps_dict[key][tic]).any():\n",
    "#             print(f\"Key: {key}, Tic: {tic}\")\n",
    "#             print(sps_dict[key][tic])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2110375a23fcab51",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for key in sps_dict.keys():\n",
    "    for tic in sps_dict[key].keys():\n",
    "        array = sps_dict[key][tic]\n",
    "        nan_indices = np.where(np.isnan(array))[0]  # Find indices of NaN values\n",
    "        for idx in nan_indices:\n",
    "            if idx < len(array) - 1:  # Check if there's a next value available\n",
    "                array[idx] = array[idx + 1]  # Replace NaN with the next value"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "701daa5159ca9346",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for key in sps_dict.keys():\n",
    "    for tic in sps_dict[key].keys():\n",
    "        if np.isnan(sps_dict[key][tic]).any():\n",
    "            print(f\"Key: {key}, Tic: {tic}\")\n",
    "            print(sps_dict[key][tic])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93bb4cb7ad4d34bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Replace nan with zeros\n",
    "\n",
    "for key in sps_dict.keys():\n",
    "    for tic in sps_dict[key].keys():\n",
    "        sps_dict[key][tic] = np.nan_to_num(sps_dict[key][tic])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48130485366a12eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Check if all tics are present in sps_dict\n",
    "# \n",
    "# for key in sps_dict.keys():\n",
    "#     for tic in data_1[\"tic\"].unique():\n",
    "#         if tic not in sps_dict[key]:\n",
    "#             print(f\"Key: {key}, Tic: {tic}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99512d89cce55d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for key in sps_dict.keys():\n",
    "    for tic in data_1[\"tic\"].unique():\n",
    "        if tic not in sps_dict[key]:\n",
    "            sps_dict[key][tic] = np.zeros(30)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414b6e1c92a4f81c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for key in sps_dict.keys():\n",
    "    for tic in data_1[\"tic\"].unique():\n",
    "        if tic not in sps_dict[key]:\n",
    "            print(f\"Key: {key}, Tic: {tic}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc42a97258c09f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sps_dict_copy = sps_dict.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6db22be8614f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "synth_tics = set(data_1[\"tic\"].unique()) - set(data_1[data_1[\"year\"].isin(range(2010, 2018))][\"tic\"].unique())\n",
    "len(synth_tics)"
   ],
   "id": "8731bb7867e99f53",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import dcor\n",
    "import itertools\n",
    "\n",
    "# Dictionary to store dcorr values for each pair of companies on each date\n",
    "dense_dependence_matrix = {}\n",
    "i = 0\n",
    "n = len(sps_dict_copy)\n",
    "\n",
    "tics = data_1[\"tic\"].unique()\n",
    "\n",
    "for date, tickers in sps_dict_copy.items():\n",
    "    print(f\"Date: {date}, {i}/{n}\")\n",
    "\n",
    "    # unique_tickers = list(tickers.keys())\n",
    "    # Use itertools to generate unique pairs of tickers\n",
    "    ticker_pairs = list(itertools.combinations(tics, 2))\n",
    "    \n",
    "    year = int(date[:4])\n",
    "    quarter = int(date[-1])\n",
    "\n",
    "    for tic1, tic2 in ticker_pairs:\n",
    "        \n",
    "        if (tic1 in synth_tics or tic2 in synth_tics) and year < 2018:\n",
    "            dense_dependence_matrix.setdefault(date, {})[f\"{tic1} {tic2}\"] = 0.0\n",
    "            dense_dependence_matrix.setdefault(date, {})[f\"{tic2} {tic1}\"] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # Get the closing price standard deviations for tic1 and tic2 on date\n",
    "        X = sps_dict_copy[date][tic1]\n",
    "        Y = sps_dict_copy[date][tic2]\n",
    "\n",
    "        # Pad the shorter array with the first element on the left side\n",
    "        len_diff = len(Y) - len(X)\n",
    "\n",
    "        if len_diff > 0:\n",
    "            X = np.pad(X, (len_diff, 0), mode='constant', constant_values=X[0])\n",
    "        elif len_diff < 0:\n",
    "            Y = np.pad(Y, (-len_diff, 0), mode='constant', constant_values=Y[0])\n",
    "\n",
    "        # Calculate the distance covariance\n",
    "        X = X.reshape(-1, 1)\n",
    "        Y = Y.reshape(-1, 1)\n",
    "\n",
    "        dcorr = dcor.distance_correlation(X, Y)\n",
    "\n",
    "        # Check if dcov is nan\n",
    "        if np.isnan(dcorr):\n",
    "            print(f\"Distance covariance between {tic1} and {tic2} on {date} is nan\")\n",
    "        else:\n",
    "            dense_dependence_matrix.setdefault(date, {})[f\"{tic1} {tic2}\"] = dcorr\n",
    "            dense_dependence_matrix.setdefault(date, {})[f\"{tic2} {tic1}\"] = dcorr\n",
    "\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad75d7c3d78a548c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(stock_data[\"tic\"].unique()), len(data_1[\"tic\"].unique())",
   "id": "2f3d02b0c2286407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_before_2018 = data_1[data_1['year'] < 2018]\n",
    "data_after_2017 = data_1[data_1['year'] >= 2018]\n",
    "data_after_2017[data_after_2017['tic'].isin(synth_tics)][['tic', 'year', 'quarter', 'pastrating']]"
   ],
   "id": "4b73823821344201",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming dense_dependence_matrix is already populated\n",
    "\n",
    "# Create a dictionary to store dependency matrices for each date along with tic indices\n",
    "dependency_matrices = {}\n",
    "tics = stock_data[\"tic\"].unique()\n",
    "tic_to_index = {tic: i for i, tic in enumerate(tics)}\n",
    "num_companies = len(tics)\n",
    "n = len(dense_dependence_matrix)\n",
    "print(tic_to_index)\n",
    "# Fill the dictionary with dependency matrices for each date\n",
    "for i, date in enumerate(dense_dependence_matrix):\n",
    "    print(f\"Date: {date}, {i}/{n}\")\n",
    "    # Extract unique company symbols from the dense_dependence_matrix for the current date\n",
    "    # tics = set(tic for pair in dense_dependence_matrix[date] for tic in pair.split())\n",
    "    # tics = data_3[\"tic\"].unique()\n",
    "\n",
    "    # Create an empty NumPy array to store dcorr values\n",
    "    \n",
    "    dependence_array = np.full((num_companies, num_companies), np.nan)\n",
    "\n",
    "    # Fill the array with dcorr values\n",
    "    for tics, dcorr in dense_dependence_matrix[date].items():\n",
    "        tic1, tic2 = tics.split()\n",
    "        #print(tic1, tic2)\n",
    "        index1, index2 = tic_to_index[tic1], tic_to_index[tic2]\n",
    "        dependence_array[index1, index2] = dcorr\n",
    "        dependence_array[index2, index1] = dcorr  # Since it's a symmetric matrix\n",
    "\n",
    "    # Replace NaN values on the diagonal with 1\n",
    "    np.fill_diagonal(dependence_array, 1)\n",
    "\n",
    "    # Store the dependency matrix and tic indices for the current date\n",
    "    dependency_matrices[date] = {'matrix': dependence_array, 'tic_indices': tic_to_index}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f473bc5cd9d2573f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj_pickle(obj, name):\n",
    "    with open(\"../Data/\" + name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# save_obj_pickle(dependency_matrices, \"dependency_matrices_new_final\")"
   ],
   "id": "d85510993172f071",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dependency_matrices[\"2022Q4\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b6be61ffbbc90e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check what years and quarters are in data_1\n",
    "\n",
    "data_1[\"year\"].unique(), data_1[\"quarter\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2928bf71a4773ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove everything after 2020Q4\n",
    "\n",
    "dependency_matrices = {k: v for k, v in dependency_matrices.items() if k <= \"2020Q4\"}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eae9e5689f7a8cbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dependency_matrices.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40b6f1b08f289b45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for date, matrix_info in dependency_matrices.items():\n",
    "    for tic, idx in matrix_info['tic_indices'].items():\n",
    "        print(f\"Date: {date}, Tic: {tic}, Index: {idx}\")"
   ],
   "id": "32c8790bf98c0785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(data_1[\"tic\"].unique())",
   "id": "bfb6b98ea57f399f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_1[(data_1['year'] == 2018) & (data_1['quarter'] == 1) & (data_1['tic'] == \"ET\")]",
   "id": "d4c70e70e4ec5e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# Assuming dependency_matrices is already populated\n",
    "\n",
    "# Create a dictionary to store graphs for each date\n",
    "dependency_graphs = {}\n",
    "i = 1\n",
    "n = len(dependency_matrices)\n",
    "tics = data_1[\"tic\"].unique()\n",
    "\n",
    "# Create graphs for each date\n",
    "for date, matrix_info in dependency_matrices.items():\n",
    "    print(f\"Date: {date}, {i}/{n}\")\n",
    "\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    year = int(date[:4])    # YYYYQN format\n",
    "    quarter = int(date[-1]) # YYYYQN format\n",
    "    \n",
    "    # Batch processing of node data for all tics\n",
    "    node_data = data_1[(data_1['year'] == year) & (data_1['quarter'] == quarter) & data_1['tic'].isin(tics)]\n",
    "    # (318, 204)\n",
    "\n",
    "    tic_to_index = matrix_info['tic_indices']\n",
    "    \n",
    "    # Add nodes with company symbols and node attributes\n",
    "    for tic, idx in matrix_info['tic_indices'].items():\n",
    "        if tic in synth_tics and year < 2018:\n",
    "            # Create a synthetic tic_node_data with the same structure as the real node_data where all values are 0 except pastrating = -1, year, quarter, tic\n",
    "            tic_node_data = pd.DataFrame(np.zeros((1, len(data_1.columns)), dtype=float), columns=data_1.columns)\n",
    "            tic_node_data['tic'] = tic\n",
    "            tic_node_data['pastrating'] = -1.0\n",
    "            tic_node_data['year'] = year\n",
    "            tic_node_data['quarter'] = quarter\n",
    "        elif tic in synth_tics and node_data[(node_data['year'] == year) & (node_data['quarter'] == quarter) & (node_data['tic'] == tic)].empty:\n",
    "            tic_node_data = pd.DataFrame(np.zeros((1, len(data_1.columns)), dtype=float), columns=data_1.columns)\n",
    "            tic_node_data['tic'] = tic\n",
    "            tic_node_data['pastrating'] = -1.0\n",
    "            tic_node_data['year'] = year\n",
    "            tic_node_data['quarter'] = quarter\n",
    "        else:\n",
    "            tic_node_data = node_data[node_data['tic'] == tic]\n",
    "        if not tic_node_data.empty:\n",
    "            node_attribute_value = tic_node_data.iloc[-1].copy()\n",
    "            node_attribute_value = node_attribute_value.drop(['datadate'])\n",
    "            G.add_node(idx)\n",
    "            nx.set_node_attributes(G, {idx: node_attribute_value})\n",
    "\n",
    "    # Add edges based on non-zero values in the matrix\n",
    "    for tic1, tic2 in combinations(tics, 2):\n",
    "        index1, index2 = tic_to_index[tic1], tic_to_index[tic2]\n",
    "        if matrix_info['matrix'][index1, index2] != 0:  # Assuming 0 represents no connection\n",
    "            G.add_edge(index1, index2, weight=matrix_info['matrix'][index1, index2])\n",
    "            # Add edge attributes\n",
    "            G[index1][index2]['weight'] = matrix_info['matrix'][index1, index2]\n",
    "\n",
    "    # Store the graph for the current date\n",
    "    dependency_graphs[date] = G\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87a7507fb70e0d69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for date, graph in dependency_graphs.items():\n",
    "    print(f\"Date: {date}\")\n",
    "    year = graph.nodes[0][\"year\"]\n",
    "    if year >= 2018:\n",
    "        for node in graph.nodes:\n",
    "            try:\n",
    "                if graph.nodes[node][\"tic\"] in synth_tics:\n",
    "                    if graph.nodes[node]['pastrating'] != -1.0:\n",
    "                        print(f\"Node: {node}, TIC: {graph.nodes[node]['tic']}, Rating: {graph.nodes[node]['pastrating']}\")\n",
    "                        print(f\"Tic to index: {tic_to_index[graph.nodes[node]['tic']]}\")\n",
    "                        print()\n",
    "            except KeyError:\n",
    "                print(\"KEY ERROR\")\n",
    "                print(f\"Node: {node}, Features: {graph.nodes[node]}\")\n",
    "                print()"
   ],
   "id": "b220d1d68bb8d017",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sample_date = list(dependency_graphs.keys())[0]\n",
    "sample_graph = dependency_graphs[sample_date]\n",
    "pos = nx.spring_layout(sample_graph)\n",
    "nx.draw_networkx(sample_graph, with_labels=True, pos=pos)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "153d69a57d160298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# sample_graph_copy = sample_graph.copy()\n",
    "# # remove nodes with no edges\n",
    "# sample_graph_copy.remove_nodes_from(list(nx.isolates(sample_graph_copy)))\n",
    "# fig, ax = plt.subplots(figsize=(80, 80))\n",
    "# # pos = nx.spring_layout(sample_graph_copy)\n",
    "# pos = nx.circular_layout(sample_graph_copy, scale=1)\n",
    "# nx.draw_networkx(sample_graph_copy, with_labels=False, pos=pos)"
   ],
   "id": "ca0153dd87ccbc63",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tmfg_corr import tmfg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a TMFG graph object\n",
    "tmfg_graph = tmfg(dependency_matrices[sample_date][\"matrix\"])\n",
    "\n",
    "# Visualize the maximum spanning tree\n",
    "# pos = nx.spring_layout(tmfg_graph)\n",
    "# nx.draw_networkx(tmfg_graph, with_labels=True, pos=pos)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos = nx.spring_layout(tmfg_graph)\n",
    "nx.draw_networkx(tmfg_graph, with_labels=True, pos=pos, ax=ax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598244831755a7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj_pickle(obj, name):\n",
    "    with open(name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_obj_pickle(tmfg_graph, \"tmfg_graph_new_3\")\n",
    "# save_obj_pickle(tic_to_index, \"tic_to_index\")\n",
    "# save_obj_pickle(sample_graph_copy, \"sample_graph_copy\")"
   ],
   "id": "b6c8f2b2bf503faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get node attributes and edge attributes for the sample graph\n",
    "\n",
    "node_attributes = nx.get_node_attributes(sample_graph, 'tic')\n",
    "\n",
    "node_attributes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea660aa435e79ddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dependency_graphs[\"2010Q1\"].nodes[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f8ac4855ef33eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "edge_attributes = nx.get_edge_attributes(sample_graph, 'weight')\n",
    "edge_attributes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9d24e0e63af7fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply the TMFG algorithm to each graph in the dependency_graphs dictionary\n",
    "\n",
    "# Create a dictionary to store the TMFG graphs for each date\n",
    "tmfg_graphs = {}\n",
    "i = 1\n",
    "n = len(dependency_graphs)\n",
    "# Apply the TMFG algorithm to each graph\n",
    "for date, matrix in dependency_matrices.items():\n",
    "    print(f\"Date: {date}, {i}/{n}\")\n",
    "    if len(matrix) > 3:\n",
    "        G = tmfg(matrix)\n",
    "    else:\n",
    "        G = dependency_graphs[date]\n",
    "\n",
    "    tmfg_graphs[date] = G\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e447116f3631718d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(tmfg_graphs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a55c0e3b3f33c62",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# How many nodes are in each graph\n",
    "\n",
    "for date, graph in tmfg_graphs.items():\n",
    "    print(f\"Date: {date}, Number of nodes: {len(graph.nodes)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d53520273ae9e52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for date, graph in tmfg_graphs.items():\n",
    "    print(f\"Date: {date}, Number of nodes: {len(graph.nodes)}\")\n",
    "    for node in graph.nodes:\n",
    "        if len(graph.nodes[node]) != 204:\n",
    "            print(node, len(graph.nodes[node]))\n",
    "    print()"
   ],
   "id": "8dbb2bf567eb13b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(tmfg_graphs[\"2020Q3\"].nodes[99].copy())",
   "id": "e5b2c463d1f1f562",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nx.set_node_attributes(tmfg_graphs[\"2020Q4\"], {99: tmfg_graphs[\"2020Q3\"].nodes[99].copy()})",
   "id": "b00d283d8b02427",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tmfg_graphs[\"2020Q4\"].nodes[99]",
   "id": "f0675b404723f0d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# What tic is 84?\n",
    "\n",
    "tic_to_index = dependency_matrices[\"2020Q4\"][\"tic_indices\"]\n",
    "tic_to_index"
   ],
   "id": "3c47aa8df3b2c245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# What nodes are missing in 2018Q1, 2018Q2, 2018Q3?\n",
    "\n",
    "for date, graph in tmfg_graphs.items():\n",
    "    if date in [\"2018Q1\", \"2018Q2\", \"2018Q3\"]:\n",
    "        tics_in_graph = set([graph.nodes[node]['tic'] for node in graph.nodes])\n",
    "        missing_tics = set(data_1[data_1['year'] == 2018]['tic'].unique()) - tics_in_graph\n",
    "        if missing_tics:\n",
    "            print(f\"Date: {date}, Missing tics: {missing_tics}\")"
   ],
   "id": "be5086abab920885",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# How many edges are in each graph\n",
    "\n",
    "for date, graph in tmfg_graphs.items():\n",
    "    print(f\"Date: {date}, Number of edges: {len(graph.edges)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe3e36ba57780bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tmfg_graphs[\"2010Q1\"].nodes[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0cc072daa9d33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_tics = set(data_1[\"tic\"].unique())\n",
    "len(all_tics)"
   ],
   "id": "a51ac6b73bd3aa03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tmfg_graphs[\"2010Q1\"].nodes[0]['tic']",
   "id": "4f771cd1d8b1257b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if all tics are present in the TMFG graphs\n",
    "\n",
    "for date, graph in tmfg_graphs.items():\n",
    "    print(graph.nodes)\n",
    "    tics_in_graph = set([graph.nodes[node]['tic'] for node in graph.nodes])\n",
    "    missing_tics = all_tics - tics_in_graph\n",
    "    if missing_tics:\n",
    "        print(f\"Date: {date}, Missing tics: {missing_tics}\")"
   ],
   "id": "2d334d283493c2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for date, graph in tmfg_graphs.items():\n",
    "    print(f\"Date: {date}\")\n",
    "    year = graph.nodes[0][\"year\"]\n",
    "    if year >= 2018:\n",
    "        for node in graph.nodes:\n",
    "            if graph.nodes[node][\"tic\"] in synth_tics:\n",
    "                if graph.nodes[node]['pastrating'] != -1.0:\n",
    "                    print(f\"Node: {node}, TIC: {graph.nodes[node]['tic']}, Rating: {graph.nodes[node]['pastrating']}\")\n",
    "                    print(f\"Tic to index: {tic_to_index[graph.nodes[node]['tic']]}\")\n",
    "                    print()"
   ],
   "id": "5e8dcb7799a7a458",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj_pickle(obj, name):\n",
    "    with open(name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj_pickle(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55fbd96c74f0da3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tics = stock_data[\"tic\"].unique()\n",
    "tic_to_index = {tic: i for i, tic in enumerate(tics)}"
   ],
   "id": "a14ae929622038fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the TMFG graphs\n",
    "\n",
    "save_obj_pickle(tmfg_graphs, \"../Data/tmfg_graphs_new_4\")\n",
    "# save_obj_pickle(tic_to_index, \"../Data/tic_to_index\")\n",
    "# save_obj_pickle(synth_tics, \"../Data/synth_tics\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cf1a95020a2a763",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
