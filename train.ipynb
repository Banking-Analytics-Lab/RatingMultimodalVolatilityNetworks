{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle, json\n",
    "\n",
    "def load_obj_pickle(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def load_obj_json(name):\n",
    "    with open(name + '.json', 'r') as f:\n",
    "        return json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "market_data = pd.read_csv('../Data/standardized_data_new.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7af4531064e245a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f74c4ab126cff9e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(list(market_data.columns))",
   "id": "bf19c8992d786cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(market_data.columns)",
   "id": "10d07172d1efd757",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(market_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff842948dd87944c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "market_data[\"pastrating\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c73952f571a1edc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "ratings = sorted(list(market_data[\"pastrating\"].unique()))",
   "metadata": {
    "collapsed": false
   },
   "id": "e77f0f087ae24271",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ratings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66bd5058a82fa416",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "ratings_to_idx = {rating: i for i, rating in enumerate(ratings)}",
   "metadata": {
    "collapsed": false
   },
   "id": "bc2dbf4b6d4612eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_to_idx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99f7616210490114",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply the ratings_to_idx dictionary to the \"pastrating\" column\n",
    "market_data[\"pastrating\"] = market_data[\"pastrating\"].apply(lambda x: ratings_to_idx[x])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5abdf88bbb171f5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "market_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7015b49aee28febc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "market_data[\"pastrating\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7da28c3cef21aa68",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tmfg_graphs = load_obj_pickle('../Data/tmfg_graphs_new_4')\n",
    "tic_to_idx = load_obj_pickle('../Data/tic_to_index')\n",
    "synth_tics = load_obj_pickle('../Data/synth_tics')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b556323bec2869bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def graph_items_to_data_seq_w_mask(graphs, unseen_node_dict, seq_len=4):\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(len(graphs) - seq_len):  # Loop through each sequence of 4 graphs\n",
    "        features_list = []\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        mask_list = []\n",
    "        unseen_mask_list = []\n",
    "\n",
    "        for j in range(seq_len):  # Extract features for 4 consecutive dates\n",
    "            date, graph = graphs[i + j]\n",
    "            node_features = [graph.nodes[k] for k in graph.nodes]\n",
    "            node_features = torch.tensor([list(node.values()) for node in node_features], dtype=torch.float32)\n",
    "            edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "            edge_attr = torch.tensor([graph.edges[edge][\"weight\"] for edge in graph.edges], dtype=torch.float32)\n",
    "            features_list.append(node_features)\n",
    "            edge_index_list.append(edge_index)\n",
    "            edge_attr_list.append(edge_attr)\n",
    "\n",
    "            # Create a mask to identify synthetic nodes based on the 'pastrating' being -1.0\n",
    "            mask = torch.tensor([graph.nodes[k]['pastrating'] != -1.0 for k in graph.nodes], dtype=torch.bool)\n",
    "            mask_list.append(mask)\n",
    "\n",
    "            # Create an unseen mask based on whether the node is part of the unseen_node_dict for the current date\n",
    "            unseen_mask = torch.tensor([k in unseen_node_dict and date in unseen_node_dict[k] for k in graph.nodes], dtype=torch.bool)\n",
    "            unseen_mask_list.append(unseen_mask)\n",
    "            \n",
    "        _, graph = graphs[i + seq_len]\n",
    "        ratings = [graph.nodes[k][\"pastrating\"] for k in graph.nodes]\n",
    "        y = torch.tensor(ratings, dtype=torch.long)\n",
    "\n",
    "        # Aggregate masks for all timesteps, considering a node synthetic if it is marked as synthetic in any timestep\n",
    "        combined_mask = torch.stack(mask_list).all(dim=0)\n",
    "        unseen_mask = torch.stack(unseen_mask_list).any(dim=0)  # True if the node is unseen at any of the time steps\n",
    "\n",
    "        data = Data(x=features_list, edge_index=edge_index_list, edge_attr=edge_attr_list, y=y, mask=combined_mask, unseen_mask=unseen_mask)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list"
   ],
   "id": "456a3dc1a43f2b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def parse_quarter(quarter):\n",
    "    print(quarter)\n",
    "    year, q = quarter.split('Q')\n",
    "    print(year, q)\n",
    "    return int(year), int(q)\n",
    "\n",
    "def quarter_sequence(year, quarter, length=4):\n",
    "    \"\"\"Generate a list of previous quarters including the current one.\"\"\"\n",
    "    quarters = []\n",
    "    for i in range(length, -1, -1):  # Include the current and go back four quarters\n",
    "        q = quarter - i\n",
    "        y = year\n",
    "        while q <= 0:\n",
    "            q += 4\n",
    "            y -= 1\n",
    "        quarters.append(f\"{y}Q{q}\")\n",
    "    return quarters\n",
    "\n",
    "def graph_items_to_data_seq_w_mask2(graphs, unseen_node_dict, seq_len=4):\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(len(graphs) - seq_len):  # Loop through each sequence of 4 graphs\n",
    "        features_list = []\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        mask_list = []\n",
    "        unseen_mask_list = []\n",
    "\n",
    "        for j in range(seq_len):  # Extract features for 4 consecutive dates\n",
    "            date, graph = graphs[i + j]\n",
    "            year, quarter = parse_quarter(date)\n",
    "            previous_quarters = quarter_sequence(year, quarter)\n",
    "\n",
    "            node_features = [graph.nodes[k] for k in graph.nodes]\n",
    "            node_features = torch.tensor([list(node.values()) for node in node_features], dtype=torch.float32)\n",
    "            edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "            edge_attr = torch.tensor([graph.edges[edge][\"weight\"] for edge in graph.edges], dtype=torch.float32)\n",
    "            features_list.append(node_features)\n",
    "            edge_index_list.append(edge_index)\n",
    "            edge_attr_list.append(edge_attr)\n",
    "\n",
    "            # mask = torch.tensor([graph.nodes[k]['pastrating'] != -1.0 for k in graph.nodes], dtype=torch.bool)\n",
    "            mask = torch.tensor([not k in unseen_node_dict for k in graph.nodes], dtype=torch.bool)\n",
    "            mask_list.append(mask)\n",
    "\n",
    "            # Update unseen_mask calculation\n",
    "            unseen_mask = torch.tensor([k in unseen_node_dict and set(previous_quarters).issubset(set(unseen_node_dict[k])) for k in graph.nodes], dtype=torch.bool)\n",
    "            unseen_mask_list.append(unseen_mask)\n",
    "\n",
    "        _, graph = graphs[i + seq_len]\n",
    "        ratings = [graph.nodes[k][\"pastrating\"] for k in graph.nodes]\n",
    "        y = torch.tensor(ratings, dtype=torch.long)\n",
    "\n",
    "        combined_mask = torch.stack(mask_list).all(dim=0)\n",
    "        unseen_mask = torch.stack(unseen_mask_list).any(dim=0)  # True if the node is unseen at any of the time steps\n",
    "\n",
    "        data = Data(x=features_list, edge_index=edge_index_list, edge_attr=edge_attr_list, y=y, mask=combined_mask, unseen_mask=unseen_mask)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n"
   ],
   "id": "9a624390c445263b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "graphs = deepcopy(tmfg_graphs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3311a4977645fc1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "graph_items = sorted(graphs.items())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22e848ae6370f9f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "unseen_nodes = defaultdict(list)\n",
    "\n",
    "for date, graph in graph_items:\n",
    "    year = int(date[:4])\n",
    "    if year >= 2018:\n",
    "        for node in graph.nodes:\n",
    "             if graph.nodes[node][\"tic\"] in synth_tics and graph.nodes[node]['pastrating'] != -1:\n",
    "                unseen_nodes[node].append(date)"
   ],
   "id": "c0b19e34c2e24798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "unseen_nodes",
   "id": "d2cdf7d8f521b74b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(unseen_nodes)",
   "id": "76aa38195cf6b3fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_quarter_cons(quarter):\n",
    "    year, q = quarter.split('Q')\n",
    "    return int(year) * 4 + int(q) - 1\n",
    "\n",
    "def is_consecutive(quarters):\n",
    "    quarters = sorted(set(parse_quarter_cons(q) for q in quarters))  # Unique and sorted\n",
    "    consecutive_sequences = []\n",
    "    current_sequence = [quarters[0]]\n",
    "\n",
    "    for i in range(1, len(quarters)):\n",
    "        if quarters[i] == quarters[i - 1] + 1:\n",
    "            current_sequence.append(quarters[i])\n",
    "        else:\n",
    "            if len(current_sequence) >= 5:\n",
    "                consecutive_sequences.extend(current_sequence)\n",
    "            current_sequence = [quarters[i]]\n",
    "\n",
    "    if len(current_sequence) >= 5:\n",
    "        consecutive_sequences.extend(current_sequence)\n",
    "\n",
    "    # Convert back to original format\n",
    "    return ['{}Q{}'.format(q // 4, q % 4 + 1) for q in consecutive_sequences]\n",
    "\n",
    "result = defaultdict(list)\n",
    "for key, values in unseen_nodes.items():\n",
    "    consecutive = is_consecutive(values)\n",
    "    if consecutive:\n",
    "        result[key] = consecutive\n",
    "        \n",
    "result"
   ],
   "id": "54f945abaf831e07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result == unseen_nodes",
   "id": "68f44f99dcd075ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Delete keys tic, year, quarter from the nodes\n",
    "for date, graph in graph_items:\n",
    "    for node in graph.nodes:\n",
    "        del graph.nodes[node][\"tic\"]\n",
    "        del graph.nodes[node][\"year\"]\n",
    "        del graph.nodes[node][\"quarter\"]"
   ],
   "id": "c7e7c37671031948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_list = graph_items_to_data_seq_w_mask2(graph_items, unseen_nodes, 4)",
   "id": "14389b7d25cc36d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(graph_items)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d026a39fff0171d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(data_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "519ee3017538a0a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_items_w_ratings, val_items_w_ratings, test_items_w_ratings = data_list[:28], data_list[28:32], data_list[32:]",
   "metadata": {
    "collapsed": false
   },
   "id": "51383ba48a8c2fd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_items_w_ratings), len(val_items_w_ratings), len(test_items_w_ratings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "810fb3da3ba4b516",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_class_weights(train_items):\n",
    "    all_labels = []\n",
    "\n",
    "    for i, data in enumerate(train_items):\n",
    "        labels = data.y.cpu().numpy()[data.mask.cpu().numpy()]\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    class_counts = Counter(all_labels)\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = {cls: total_samples / (len(class_counts) * count) for cls, count in class_counts.items()}\n",
    "\n",
    "    return class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e394ea468a0435",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class_weights = compute_class_weights(data_list[:len(data_list)-1])",
   "metadata": {
    "collapsed": false
   },
   "id": "1ce17b2e40f9d33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "class_weights",
   "id": "1b497772d2839ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class_weights = [class_weights[rating] for rating in sorted(class_weights)]\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b622b96081dd8f7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff16142053183c79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "from torcheval.metrics.functional import multiclass_auprc\n",
    "from torcheval.metrics import MulticlassAUPRC\n",
    "from rgnns import GConvLSTMModel2, GConvGRUModel2\n",
    "\n",
    "\n",
    "def bootstrap_preds_multiclass(probs, true_labels, num_classes=8, num_boot=10000):\n",
    "    boot_means_auc = np.zeros(num_boot)\n",
    "    boot_means_auprc = np.zeros(num_boot)\n",
    "    boot_means_f1 = np.zeros(num_boot)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    for i in range(num_boot):\n",
    "        # Generate indices for resampling\n",
    "        indices = np.random.choice(range(len(probs)), size=len(probs), replace=True)\n",
    "        # Resample labels and predictions\n",
    "        resampled_labels = true_labels[indices]\n",
    "        resampled_probs = probs[indices]\n",
    "\n",
    "        # Recalculate AUC, AUPRC, and F1 for resampled data\n",
    "        resampled_labels_binarized = label_binarize(resampled_labels, classes=np.arange(num_classes))\n",
    "\n",
    "        try:\n",
    "            # AUC calculation\n",
    "            auc_score = roc_auc_score(resampled_labels_binarized, resampled_probs, average='macro', multi_class='ovr')\n",
    "            boot_means_auc[i] = auc_score\n",
    "\n",
    "            # AUPRC calculation\n",
    "            boot_means_auprc[i] = multiclass_auprc(torch.tensor(resampled_probs), torch.tensor(resampled_labels), num_classes=num_classes)\n",
    "\n",
    "            # F1 calculation\n",
    "            preds_resampled = resampled_probs.argmax(axis=1)\n",
    "            _, _, f1_score, _ = precision_recall_fscore_support(resampled_labels, preds_resampled, average='macro', zero_division=0)\n",
    "            boot_means_f1[i] = f1_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in bootstrap iteration {i}: {e}\")\n",
    "            boot_means_auc[i] = 0\n",
    "            boot_means_auprc[i] = 0\n",
    "            boot_means_f1[i] = 0\n",
    "\n",
    "    return boot_means_auc, boot_means_auprc, boot_means_f1\n",
    "\n",
    "def train(model, loader, criterion, optimizer, h0_n, h0_g):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(model.device)\n",
    "\n",
    "        if type(model) is RGNN_RNN:\n",
    "            out, h0_n, h0_g = model(batch, h0_n, h0_g)\n",
    "            if type(model.GNN) is GConvLSTMModel:\n",
    "                h0_g = [tuple(h.detach() for h in layer) for layer in h0_g]\n",
    "            else:\n",
    "                h0_g = [h.detach() for h in h0_g]\n",
    "        elif type(model) is GConvLSTMModel2:\n",
    "            out, h0_g = model(batch.edge_index, batch.edge_attr, h0_g)\n",
    "            h0_g = [tuple(h.detach() for h in layer) for layer in h0_g]\n",
    "        elif type(model) is GConvGRUModel2:\n",
    "            out, h0_g = model(batch.edge_index, batch.edge_attr, h0_g)\n",
    "            h0_g = [h.detach() for h in h0_g]\n",
    "        else: # RNN\n",
    "            out, h0_n = model(batch, h0_n, False)\n",
    "\n",
    "        loss = criterion(out[batch.mask], batch.y[batch.mask])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        correct_predictions += (out[batch.mask].argmax(dim=1) == batch.y[batch.mask]).sum().item()\n",
    "        total_samples += batch.y[batch.mask].shape[0]   # batch.mask.sum().item()\n",
    "        \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return avg_loss, accuracy, h0_n, h0_g\n",
    "\n",
    "def evaluate(model, loader, criterion, h0_n, h0_g, inferencing=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probs = []\n",
    "    unseen_predictions = []\n",
    "    unseen_true_labels = []\n",
    "    unseen_probs = []\n",
    "    unseen_correct_predictions = 0\n",
    "    unseen_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            batch = batch.to(model.device)\n",
    "\n",
    "            if type(model) is RGNN_RNN:\n",
    "                out, h0_n, h0_g = model(batch, h0_n, h0_g)\n",
    "                if type(model.GNN) is GConvLSTMModel:\n",
    "                    h0_g = [tuple(h.detach() for h in layer) for layer in h0_g]\n",
    "                else:\n",
    "                    h0_g = [h.detach() for h in h0_g]\n",
    "            elif type(model) is GConvLSTMModel2:\n",
    "                out, h0_g = model(batch.edge_index, batch.edge_attr, h0_g)\n",
    "                h0_g = [tuple(h.detach() for h in layer) for layer in h0_g]\n",
    "            elif type(model) is GConvGRUModel2:\n",
    "                out, h0_g = model(batch.edge_index, batch.edge_attr, h0_g)\n",
    "                h0_g = [h.detach() for h in h0_g]\n",
    "            else: # RNN\n",
    "                out, h0_n = model(batch, h0_n, False)\n",
    "    \n",
    "            loss = criterion(out[batch.mask], batch.y[batch.mask])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_pred = out[batch.mask].argmax(dim=1)\n",
    "            y_true = batch.y[batch.mask]\n",
    "            correct_predictions += (y_pred == y_true).sum().item()\n",
    "            total_samples += batch.y[batch.mask].shape[0]\n",
    "            \n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "            true_labels.append(y_true.cpu().numpy())\n",
    "            probs.append(out[batch.mask].cpu().numpy())\n",
    "  \n",
    "            # Inference for unseen nodes\n",
    "            if inferencing:\n",
    "                y_pred_unseen = out[batch.unseen_mask].argmax(dim=1)\n",
    "                y_true_unseen = batch.y[batch.unseen_mask]\n",
    "                unseen_correct_predictions += (y_pred_unseen == y_true_unseen).sum().item()\n",
    "                unseen_total_samples += batch.unseen_mask.sum().item()\n",
    "                unseen_predictions.append(y_pred_unseen.cpu().numpy())\n",
    "                unseen_true_labels.append(y_true_unseen.cpu().numpy())\n",
    "                unseen_probs.append(out[batch.unseen_mask].cpu().numpy())\n",
    "\n",
    "    # Flatten true labels and predictions\n",
    "    true_labels_flat = np.concatenate(true_labels)\n",
    "    probs_flat = np.concatenate(probs)\n",
    "    predictions_flat = np.concatenate(predictions)\n",
    "\n",
    "    # Binarize true labels for multiclass classification\n",
    "    true_labels_binarized = label_binarize(true_labels_flat, classes=np.unique(true_labels_flat))\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    try:\n",
    "        # auc = roc_auc_score(true_labels_binarized, probs_flat[:, 1:], average='macro', multi_class='ovr')\n",
    "        auc = roc_auc_score(true_labels_binarized, probs_flat, average='macro', multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc = 0\n",
    "\n",
    "    # Calculate AUPRC score\n",
    "    try:\n",
    "        auprc = multiclass_auprc(torch.tensor(probs_flat), torch.tensor(true_labels_flat), num_classes=8)\n",
    "    except ValueError:\n",
    "        auprc = 0\n",
    "\n",
    "    if inferencing:\n",
    "        unseen_predictions_flat = np.concatenate(unseen_predictions)\n",
    "        unseen_true_labels_flat = np.concatenate(unseen_true_labels)\n",
    "        unseen_probs_flat = np.concatenate(unseen_probs)\n",
    "        accuracy_unseen = unseen_correct_predictions / unseen_total_samples\n",
    "        precision_unseen, recall_unseen, f1_unseen, _ = precision_recall_fscore_support(unseen_true_labels_flat, unseen_predictions_flat, average='macro', zero_division=0)\n",
    "\n",
    "        true_labels_binarized_unseen = label_binarize(unseen_true_labels_flat, classes=np.unique(unseen_true_labels_flat))\n",
    "        auc_unseen = roc_auc_score(true_labels_binarized_unseen, unseen_probs_flat, average='macro', multi_class='ovr')\n",
    "        auprc_unseen = multiclass_auprc(torch.tensor(unseen_probs_flat), torch.tensor(unseen_true_labels_flat), num_classes=8)\n",
    "\n",
    "        auc_boot_means, auprc_boot_means, f1_boot_means = bootstrap_preds_multiclass(probs_flat, true_labels_flat, num_classes=8, num_boot=10000)\n",
    "        auc_boot_means_unseen, auprc_boot_means_unseen, f1_boot_means_unseen = bootstrap_preds_multiclass(unseen_probs_flat, unseen_true_labels_flat, num_classes=8, num_boot=10000)\n",
    "    else:\n",
    "        auc_boot_means = auprc_boot_means = f1_boot_means = np.zeros(1)\n",
    "        auc_boot_means_unseen = auprc_boot_means_unseen = f1_boot_means_unseen = np.zeros(1)\n",
    "        accuracy_unseen = precision_unseen = recall_unseen = f1_unseen = auc_unseen = auprc_unseen = 0\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predictions_flat, average='macro', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc, auprc, auc_boot_means, auprc_boot_means, f1_boot_means, accuracy_unseen, precision_unseen, recall_unseen, f1_unseen, auc_unseen, auprc_unseen, auc_boot_means_unseen, auprc_boot_means_unseen, f1_boot_means_unseen"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86ab4b0bacb5b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97c518430d2b8d15",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from models import RGNN_RNN, RNN\n",
    "from rnns import LSTMModel, GRUModel, TransformerModel\n",
    "from rgnns import GConvGRUModel, GConvLSTMModel, GConvGRUModel2, GConvLSTMModel2\n",
    "import torch.nn as nn\n",
    "\n",
    "in_channels = 201\n",
    "rnn_hidden_channels = 16\n",
    "gnn_hidden_channels = 64\n",
    "num_classes = 8\n",
    "num_heads = 4\n",
    "num_gnn_layers = 2\n",
    "num_rnn_layers = 8\n",
    "edge_dim = 1\n",
    "num_nodes = train_items_w_ratings[0].x[0].shape[0]\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Multi-modal RGNN-RNN model\n",
    "model = RGNN_RNN(num_features=in_channels, rnn_hidden_dim=rnn_hidden_channels, gnn_hidden_dim=gnn_hidden_channels,  num_classes=num_classes, num_gnn_layers=num_gnn_layers, num_rnn_layers=num_rnn_layers, edge_dim=edge_dim, num_heads=num_heads, num_nodes=num_nodes, rgnn_model=GConvLSTMModel, rnn_model=LSTMModel).cuda()\n",
    "\n",
    "# Uni-modal RNN model\n",
    "# model = RNN(num_features=in_channels, hidden_dim=rnn_hidden_channels, num_classes=num_classes, num_rnn_layers=num_rnn_layers, num_nodes=num_nodes, rnn_model=GRUModel).cuda()\n",
    "\n",
    "# Uni-modal RGNN models\n",
    "# model = GConvLSTMModel2(input_dim=in_channels, hidden_dim=gnn_hidden_channels, output_dim=num_classes, n_layers=num_gnn_layers, n_nodes=num_nodes).cuda()\n",
    "\n",
    "# model = GConvGRUModel2(input_dim=in_channels, hidden_dim=gnn_hidden_channels, output_dim=num_classes, n_layers=num_gnn_layers, n_nodes=num_nodes).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1, min_lr=1e-6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0f1c7aa47585293",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "aucs = []\n",
    "aucprcs = []\n",
    "val_unseen_aucs = []\n",
    "val_unseen_aucprcs = []\n",
    "attention_scores = []\n",
    "h0_n, h0_g = None, None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_accuracy, h0_n, h0_g = train(model, train_items_w_ratings, criterion, optimizer, h0_n, h0_g)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy) \n",
    "    print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "    val_loss, val_accuracy, precision, recall, f1, auc, auprc, _, _, _, _, _, _, _, _, _, _, _, _, = evaluate(model, val_items_w_ratings, criterion, h0_n, h0_g, False)\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    aucs.append(auc)\n",
    "    aucprcs.append(auprc)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}, AUPRC: {auprc:.4f}')\n",
    "\n",
    "# Testing loop\n",
    "test_loss, test_accuracy, precision, recall, f1, auc, auprc, auc_boot_means, auprc_boot_means, f1_boot_means, accuracy_unseen, precision_unseen, recall_unseen, f1_unseen, auc_unseen, auprc_unseen, auc_boot_means_unseen, auprc_boot_means_unseen, f1_boot_means_unseen = evaluate(model, test_items_w_ratings[:-1], criterion, h0_n, h0_g, True)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}, AUPRC: {auprc:.4f}, Accuracy Unseen: {accuracy_unseen:.4f}, Precision Unseen: {precision_unseen:.4f}, Recall Unseen: {recall_unseen:.4f}, F1 Unseen: {f1_unseen:.4f}, AUC Unseen: {auc_unseen:.4f}, AUPRC Unseen: {auprc_unseen:.4f}')\n",
    "print(f\"Mean AUC: {auc_boot_means.mean():.4f}, Mean AUPRC: {auprc_boot_means.mean():.4f}, Mean F1: {f1_boot_means.mean():.4f}, Mean AUC Unseen: {auc_boot_means_unseen.mean():.4f}, Mean AUPRC Unseen: {auprc_boot_means_unseen.mean():.4f}, Mean F1 Unseen: {f1_boot_means_unseen.mean():.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b562aea040098b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8c7e9a5ea3cfc49c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the auc results with 95% confidence interval\n",
    "auc_lower = np.percentile(auc_boot_means, 2.5)\n",
    "auc_upper = np.percentile(auc_boot_means, 97.5)\n",
    "auc_mean = auc_boot_means.mean()\n",
    "auc_mean_dist_lower = auc_mean - auc_lower\n",
    "auc_mean_dist_upper = auc_upper - auc_mean\n",
    "\n",
    "# Get the auprc results with 95% confidence interval\n",
    "auprc_lower = np.percentile(auprc_boot_means, 2.5)\n",
    "auprc_upper = np.percentile(auprc_boot_means, 97.5)\n",
    "auprc_mean = auprc_boot_means.mean()\n",
    "auprc_mean_dist_lower = auprc_mean - auprc_lower\n",
    "auprc_mean_dist_upper = auprc_upper - auprc_mean\n",
    "\n",
    "# Get the f1 results with 95% confidence interval\n",
    "f1_lower = np.percentile(f1_boot_means, 2.5)\n",
    "f1_upper = np.percentile(f1_boot_means, 97.5)\n",
    "f1_mean = f1_boot_means.mean()\n",
    "f1_mean_dist_lower = f1_mean - f1_lower\n",
    "f1_mean_dist_upper = f1_upper - f1_mean\n",
    "\n",
    "# Get the auc results with 95% confidence interval for unseen nodes\n",
    "auc_unseen_lower = np.percentile(auc_boot_means_unseen, 2.5)\n",
    "auc_unseen_upper = np.percentile(auc_boot_means_unseen, 97.5)\n",
    "auc_unseen_mean = auc_boot_means_unseen.mean()\n",
    "auc_unseen_mean_dist_lower = auc_unseen_mean - auc_unseen_lower\n",
    "auc_unseen_mean_dist_upper = auc_unseen_upper - auc_unseen_mean\n",
    "\n",
    "# Get the auprc results with 95% confidence interval for unseen nodes\n",
    "auprc_unseen_lower = np.percentile(auprc_boot_means_unseen, 2.5)\n",
    "auprc_unseen_upper = np.percentile(auprc_boot_means_unseen, 97.5)\n",
    "auprc_unseen_mean = auprc_boot_means_unseen.mean()\n",
    "auprc_unseen_mean_dist_lower = auprc_unseen_mean - auprc_unseen_lower\n",
    "auprc_unseen_mean_dist_upper = auprc_unseen_upper - auprc_unseen_mean\n",
    "\n",
    "# Get the f1 results with 95% confidence interval for unseen nodes\n",
    "f1_unseen_lower = np.percentile(f1_boot_means_unseen, 2.5)\n",
    "f1_unseen_upper = np.percentile(f1_boot_means_unseen, 97.5)\n",
    "f1_unseen_mean = f1_boot_means_unseen.mean()\n",
    "f1_unseen_mean_dist_lower = f1_unseen_mean - f1_unseen_lower\n",
    "f1_unseen_mean_dist_upper = f1_unseen_upper - f1_unseen_mean\n",
    "\n",
    "print(f\"AUC: {auc_mean:.4f} ({auc_mean_dist_lower:.4f}, {auc_mean_dist_upper:.4f})\")\n",
    "print(f\"AUPRC: {auprc_mean:.4f} ({auprc_mean_dist_lower:.4f}, {auprc_mean_dist_upper:.4f})\")\n",
    "print(f\"F1: {f1_mean:.4f} ({f1_mean_dist_lower:.4f}, {f1_mean_dist_upper:.4f})\")\n",
    "print(f\"AUC Unseen: {auc_unseen_mean:.4f} ({auc_unseen_mean_dist_lower:.4f}, {auc_unseen_mean_dist_upper:.4f})\")\n",
    "print(f\"AUPRC Unseen: {auprc_unseen_mean:.4f} ({auprc_unseen_mean_dist_lower:.4f}, {auprc_unseen_mean_dist_upper:.4f})\")\n",
    "print(f\"F1 Unseen: {f1_unseen_mean:.4f} ({f1_unseen_mean_dist_lower:.4f}, {f1_unseen_mean_dist_upper:.4f})\")"
   ],
   "id": "1894ca282bf859f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
